apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: gbt-100000000-19-checkpoint
spec:
  type: Scala
  mode: cluster
  image: mcd01/spark:v3.0.0-servlet
  imagePullPolicy: Always
  sparkConf:
    # hadoop
    "spark.hadoop.fs.defaultFS": hdfs://130.149.249.46:9000
    # event log
    "spark.eventLog.enabled": "true"
    "spark.eventLog.dir": hdfs:///spark/job-event-log/
    "spark.eventLog.logStageExecutorMetrics": "true"
    # set max resultsize to unlimited
    "spark.driver.maxResultSize": "0"
    # metric monitoring + k8s monitoring
    "spark.metrics.conf": "/etc/metrics/conf/metrics.properties"
    "spark.ui.prometheus.enabled": "true"
    "spark.metrics.appStatusSource.enabled": "true"
    "spark.sql.streaming.metricsEnabled": "true"
    "spark.metrics.staticSources.enabled": "true"
    "spark.metrics.executorMetricsSource.enabled": "true"
    "spark.executor.processTreeMetrics.enabled": "true"
    "spark.kubernetes.driver.annotation.prometheus.io/scrape": "true"
    "spark.kubernetes.driver.annotation.prometheus.io/path": "/metrics/prometheus/ | /metrics/executors/prometheus/"
    "spark.kubernetes.driver.annotation.prometheus.io/port": "4040"
  mainClass: de.tu_berlin.dos.arm.spark_utils.jobs.GradientBoostedTrees
  mainApplicationFile: hdfs:///jar-files/spark-checkpoint-workloads-1.0-SNAPSHOT-jar-with-dependencies.jar
  arguments: [ "--iterations", "10", "--checkpoint", "1", "--checkpoint-interval", "3", "hdfs:///spark/sgd/100000000_19.txt"]
  sparkVersion: "3.0.0"
  restartPolicy:
    type: Never
  driver:
    cores: 4
    memory: 4096m
    labels:
      version: 3.0.0
    serviceAccount: "spark-operator"
  executor:
    instances: 10
    cores: 4
    memory: 8048m
    # keep executors pods after spark app is terminated
    deleteOnTermination: True
    labels:
      version: 3.0.0