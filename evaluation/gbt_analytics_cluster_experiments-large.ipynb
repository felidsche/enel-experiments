{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosted Trees (GBT) & Analytics Workload Experiment setup\n",
    "\n",
    "- Date: 25.09.2021\n",
    "- Execution: DOS ARM Spark K8s cluster\n",
    "- Checkpoint Interval: every 2 iterations\n",
    "- Only the Analytics workloads wrote files on HDFS\n",
    "\n",
    "\n",
    "| **Workload App**         | **Checkpoint**     | **Input size** | **Drivers**         | **Driver Cores**     |**Driver Memory** |**Executors** |**Executors Cores** |**Executors Memory** |**Iterations** | **Checkpoint size** |\n",
    "|--------------|-----------|------------|--------------|-----------|------------|------------|------------|------------|------------|------------|\n",
    "| gbt-9000000-10 | NO      | 35 GB        | 1 | 4      | 4 GB        | 10        | 4        | 8 GB    |5        | / |\n",
    "| gbt-9000000-10-checkpoint      | YES  | 35 GB       | 1      | 4  | 4 GB       | 10        | 4        | 8 GB | 5 | 0  |\n",
    "| analytics-1gb      | NO  | 30 GB       | 1      | 4  | 4 GB       | 10       | 4        | 8 GB | 5  | /  |\n",
    "| analytics-1gb-checkpoint | YES  | 30 GB       | 1      | 4  | 4 GB       | 10       | 4        | 8 GB | 5 | 40GB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast(gbt_df: DataFrame) -> DataFrame:\n",
    "    # cast numeric and datetime columns\n",
    "    datetime_pattern = \"yyyy-MM-dd'T'HH:mm:ss.SSSz\"\n",
    "\n",
    "    gbt_df = gbt_df.select(\n",
    "        \"*\",\n",
    "        gbt_df.taskIndex.cast(T.IntegerType()).alias(\"taskIndex_\"),\n",
    "        gbt_df.rddId.cast(T.IntegerType()).alias(\"rddId_\"),\n",
    "        gbt_df.taskAttempt.cast(T.IntegerType()).alias(\"taskAttempt_\"),\n",
    "        gbt_df.taskDuration.cast(T.FloatType()).alias(\"taskDuration_\"),\n",
    "        gbt_df.taskExecutorId.cast(T.IntegerType()).alias(\"taskExecutorId_\"),\n",
    "        gbt_df.tcMs.cast(T.FloatType()).alias(\"tcMs_\"),\n",
    "        F.to_timestamp(gbt_df.submissionTime, datetime_pattern).alias(\"submissionTime_\"),\n",
    "        F.to_timestamp(gbt_df.firstTaskLaunchedTime, datetime_pattern).alias(\"firstTaskLaunchedTime_\"),\n",
    "        F.to_timestamp(gbt_df.completionTime, datetime_pattern).alias(\"completionTime_\"),\n",
    "        F.to_timestamp(gbt_df.taskLaunchTime, datetime_pattern).alias(\"taskLaunchTime_\")\n",
    "    ).drop(\"taskIndex\", \"rddId\", \"taskId\", \"tcMs\",\n",
    "           \"taskAttempt\", \"taskDuration\", \"taskExecutorId\", \"submissionTime\",\n",
    "           \"firstTaskLaunchedTime\", \"completionTime\", \"taskLaunchTime\")\n",
    "    \n",
    "    # rename columns\n",
    "    new_column_names = [c.rstrip(\"_\") for c in gbt_df.columns]\n",
    "    gbt_df = gbt_df.toDF(*new_column_names)\n",
    "\n",
    "    gbt_df = gbt_df.withColumnRenamed(\"_c0\", \"id\")\n",
    "    \n",
    "    return gbt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_df = spark.read.csv(\n",
    "    path=\"../output/cluster/GradientBoostedTrees/spark-62411c71565b4cf8b0e316c07c8a4d5f_normal.csv\",\n",
    "    sep=\",\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_check_df = spark.read.csv(\n",
    "    path=\"../output/cluster/GradientBoostedTrees/spark-6f87c902a05c4a52b460bc8555ca42a0_checkpoint.csv\",\n",
    "    sep=\",\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 join each 5 execution of the `Analytics` workload DataFrames together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows_before_join: 1392\n",
      "rows_after_join: 1392\n"
     ]
    }
   ],
   "source": [
    "first_analytics_run_path = \"../output/cluster/Analytics/spark-9078bc6ddb3a4c0b913072d827c939bc_normal.csv\"\n",
    "# key: the sequence of execution, value: the path of the output of run_experiments.py\n",
    "analytics_df_runs = {\n",
    "    \"2\": \"../output/cluster/Analytics/spark-827da77e3d5946b395e7359bb0534f22_normal.csv\",\n",
    "    \"3\": \"../output/cluster/Analytics/spark-ab3aa599abf141568bed1c53aee2f842_normal.csv\",\n",
    "    \"4\": \"../output/cluster/Analytics/spark-f9330f93633948d195315dbeba6313f2_normal.csv\",\n",
    "    \"5\": \"../output/cluster/Analytics/spark-af529ab0a80b48168d3c95c60bf7bca7_normal.csv\"\n",
    "}\n",
    "analytics_df = spark.read.csv(\n",
    "    path=first_analytics_run_path,\n",
    "    sep=\",\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "analytics_df = cast(analytics_df)\n",
    "\n",
    "rows_before_join = analytics_df.count()\n",
    "print(f\"rows_before_join: {rows_before_join}\")\n",
    "\n",
    "for key, value in analytics_df_runs.items():\n",
    "    df_inc = spark.read.csv(\n",
    "        path=value,\n",
    "        sep=\",\",\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    \n",
    "    df_inc = cast(df_inc)\n",
    "    \n",
    "    # add suffix to the incrementing dataframe\n",
    "    df_inc = df_inc.select([F.col(c).alias(f\"{c}_{key}\") for c in df_inc.columns])\n",
    "    \n",
    "    # add join column\n",
    "    df_inc = df_inc.withColumn(\"id\", F.monotonically_increasing_id())\n",
    "    \n",
    "    # of these columns, we use the mean of the 5 runs\n",
    "    mean_cols = [\n",
    "        \"id\",\n",
    "        df_inc.columns[6],  # numCompleteTasks\n",
    "        df_inc.columns[7],  # numFailedTasks\n",
    "        df_inc.columns[8],  # numKilledTasks\n",
    "        df_inc.columns[13],  # taskAttempt\n",
    "        df_inc.columns[14],  # taskDuration\n",
    "        df_inc.columns[16]  # tcMs\n",
    "    ]\n",
    "    \n",
    "    analytics_df = analytics_df.join(\n",
    "        df_inc.select(mean_cols),\n",
    "        on=\"id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "rows_after_join = analytics_df.count()\n",
    "print(f\"rows_after_join: {rows_after_join}\")\n",
    "\n",
    "assert rows_after_join == rows_before_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows_before_join: 10152\n",
      "rows_after_join: 10152\n"
     ]
    }
   ],
   "source": [
    "first_run_path = \"../output/cluster/Analytics/spark-d493e730d6be481896910ff2a003db4e_checkpoint.csv\"\n",
    "# key: the sequence of execution, value: the path of the output of run_experiments.py\n",
    "analytics_check_df_runs = {\n",
    "    \"2\": \"../output/cluster/Analytics/spark-86ce2033320d452ebfb4c69e0d5aaaad_checkpoint.csv\",\n",
    "    \"3\": \"../output/cluster/Analytics/spark-581a0b09967648cca77d0084ed25af2f_checkpoint.csv\",\n",
    "    \"4\": \"../output/cluster/Analytics/spark-83e529840d2c4c0eb550105373fed434_checkpoint.csv\",\n",
    "    \"5\": \"../output/cluster/Analytics/spark-4c736126ec9a4b72a96a76c1155cd03e_checkpoint.csv\"\n",
    "}\n",
    "analytics_check_df = spark.read.csv(\n",
    "    path=first_run_path,\n",
    "    sep=\",\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "analytics_check_df = cast(analytics_check_df)\n",
    "\n",
    "rows_before_join = analytics_check_df.count()\n",
    "print(f\"rows_before_join: {rows_before_join}\")\n",
    "\n",
    "for key, value in analytics_check_df_runs.items():\n",
    "    df = spark.read.csv(\n",
    "        path=value,\n",
    "        sep=\",\",\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    \n",
    "    df = cast(df)\n",
    "    \n",
    "    \n",
    "    # add suffix to the incrementing dataframe\n",
    "    df = df.select([F.col(c).alias(f\"{c}_{key}\") for c in df.columns])\n",
    "    \n",
    "    # add join column\n",
    "    df = df.withColumn(\"id\", F.monotonically_increasing_id())\n",
    "    \n",
    "    # of these columns, we use the mean of the 5 runs\n",
    "    mean_cols = [\n",
    "        \"id\",\n",
    "        df.columns[6],  # numCompleteTasks\n",
    "        df.columns[7],  # numFailedTasks\n",
    "        df.columns[8],  # numKilledTasks\n",
    "        df.columns[13],  # taskAttempt\n",
    "        df.columns[14],  # taskDuration\n",
    "        df.columns[16]  # tcMs\n",
    "    ]\n",
    "    \n",
    "    analytics_check_df = analytics_check_df.join(\n",
    "        df.select(mean_cols),\n",
    "        on=\"id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "\n",
    "rows_after_join = analytics_check_df.count()\n",
    "print(f\"rows_after_join: {rows_after_join}\")\n",
    "\n",
    "assert rows_after_join == rows_before_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, status: string, stageId: int, attemptId: int, numTasks: int, numActiveTasks: int, numCompleteTasks: int, numFailedTasks: int, numKilledTasks: int, name: string, rddIds: string, taskIndex: int, rddId: int, taskAttempt: int, taskDuration: float, taskExecutorId: int, tcMs: float, submissionTime: timestamp, firstTaskLaunchedTime: timestamp, completionTime: timestamp, taskLaunchTime: timestamp, numCompleteTasks_2: int, numFailedTasks_2: int, numKilledTasks_2: int, taskAttempt_2: int, taskDuration_2: float, tcMs_2: float, numCompleteTasks_3: int, numFailedTasks_3: int, numKilledTasks_3: int, taskAttempt_3: int, taskDuration_3: float, tcMs_3: float, numCompleteTasks_4: int, numFailedTasks_4: int, numKilledTasks_4: int, taskAttempt_4: int, taskDuration_4: float, tcMs_4: float, numCompleteTasks_5: int, numFailedTasks_5: int, numKilledTasks_5: int, taskAttempt_5: int, taskDuration_5: float, tcMs_5: float]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analytics_check_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 calculate the mean of the values that were collected over all 5 runs in the `Analytics` workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytics_df = analytics_df.withColumn(\n",
    "    \"meanNumCompleteTasks\",\n",
    "    (F.col(\"numCompleteTasks\") + F.col(\"numCompleteTasks_2\") + F.col(\"numCompleteTasks_3\") + F.col(\"numCompleteTasks_4\") + F.col(\"numCompleteTasks_5\") / 5)\n",
    ")\n",
    "\n",
    "analytics_df = analytics_df.withColumn(\n",
    "    \"meanNumFailedTasks\",\n",
    "    (F.col(\"numFailedTasks\") + F.col(\"numFailedTasks_2\") + F.col(\"numFailedTasks_3\") + F.col(\"numFailedTasks_4\") + F.col(\"numFailedTasks_5\") / 5)\n",
    ")\n",
    "\n",
    "analytics_df = analytics_df.withColumn(\n",
    "    \"meanNumKilledTasks\",\n",
    "    (F.col(\"numKilledTasks\") + F.col(\"numKilledTasks_2\") + F.col(\"numKilledTasks_3\") + F.col(\"numKilledTasks_4\") + F.col(\"numKilledTasks_5\") / 5)\n",
    ")\n",
    "\n",
    "analytics_df = analytics_df.withColumn(\n",
    "    \"meanTaskAttempt\",\n",
    "    (F.col(\"taskAttempt\") + F.col(\"taskAttempt_2\") + F.col(\"taskAttempt_3\") + F.col(\"taskAttempt_4\") + F.col(\"taskAttempt_5\") / 5)\n",
    ")\n",
    "\n",
    "analytics_df = analytics_df.withColumn(\n",
    "    \"meanTaskAttempt\",\n",
    "    (F.col(\"taskAttempt\") + F.col(\"taskAttempt_2\") + F.col(\"taskAttempt_3\") + F.col(\"taskAttempt_4\") + F.col(\"taskAttempt_5\") / 5)\n",
    ")\n",
    "\n",
    "analytics_df = analytics_df.withColumn(\n",
    "    \"meanTaskDuration\",\n",
    "    (F.col(\"taskDuration\") + F.col(\"taskDuration_2\") + F.col(\"taskDuration_3\") + F.col(\"taskDuration_4\") + F.col(\"taskDuration_5\") / 5)\n",
    ")\n",
    "\n",
    "analytics_df = analytics_df.withColumn(\n",
    "    \"meanTcMs\",\n",
    "    (F.col(\"tcMs\") + F.col(\"tcMs_2\") + F.col(\"tcMs_3\") + F.col(\"tcMs_4\") + F.col(\"tcMs_5\") / 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytics_check_df = analytics_check_df.withColumn(\n",
    "    \"meanNumCompleteTasks\",\n",
    "    (F.col(\"numCompleteTasks\") + F.col(\"numCompleteTasks_2\") + F.col(\"numCompleteTasks_3\") + F.col(\"numCompleteTasks_4\") + F.col(\"numCompleteTasks_5\") / 5)\n",
    ")\n",
    "\n",
    "analytics_check_df = analytics_check_df.withColumn(\n",
    "    \"meanNumFailedTasks\",\n",
    "    (F.col(\"numFailedTasks\") + F.col(\"numFailedTasks_2\") + F.col(\"numFailedTasks_3\") + F.col(\"numFailedTasks_4\") + F.col(\"numFailedTasks_5\") / 5)\n",
    ")\n",
    "\n",
    "analytics_check_df = analytics_check_df.withColumn(\n",
    "    \"meanNumKilledTasks\",\n",
    "    (F.col(\"numKilledTasks\") + F.col(\"numKilledTasks_2\") + F.col(\"numKilledTasks_3\") + F.col(\"numKilledTasks_4\") + F.col(\"numKilledTasks_5\") / 5)\n",
    ")\n",
    "\n",
    "analytics_check_df = analytics_check_df.withColumn(\n",
    "    \"meanTaskAttempt\",\n",
    "    (F.col(\"taskAttempt\") + F.col(\"taskAttempt_2\") + F.col(\"taskAttempt_3\") + F.col(\"taskAttempt_4\") + F.col(\"taskAttempt_5\") / 5)\n",
    ")\n",
    "\n",
    "analytics_check_df = analytics_check_df.withColumn(\n",
    "    \"meanTaskAttempt\",\n",
    "    (F.col(\"taskAttempt\") + F.col(\"taskAttempt_2\") + F.col(\"taskAttempt_3\") + F.col(\"taskAttempt_4\") + F.col(\"taskAttempt_5\") / 5)\n",
    ")\n",
    "\n",
    "analytics_check_df = analytics_check_df.withColumn(\n",
    "    \"meanTaskDuration\",\n",
    "    (F.col(\"taskDuration\") + F.col(\"taskDuration_2\") + F.col(\"taskDuration_3\") + F.col(\"taskDuration_4\") + F.col(\"taskDuration_5\") / 5)\n",
    ")\n",
    "\n",
    "analytics_check_df = analytics_check_df.withColumn(\n",
    "    \"meanTcMs\",\n",
    "    (F.col(\"tcMs\") + F.col(\"tcMs_2\") + F.col(\"tcMs_3\") + F.col(\"tcMs_4\") + F.col(\"tcMs_5\") / 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_df = cast(gbt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_check_df = cast(gbt_check_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Data summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `GBT`\n",
    "- 16 as maximum for `numCompleteTasks` make sense since 4 executors with 4 cores each can compute maximum 16 tasks in parallel\n",
    "- 4 as maximum for `numFailedTasks` means that no cores were available anymore, thus the executor failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_df.select(\"numCompleteTasks\", \"numFailedTasks\", \"numKilledTasks\", \"taskAttempt\", \"taskDuration\").summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_check_df.select(\"numCompleteTasks\", \"numFailedTasks\", \"numKilledTasks\", \"taskAttempt\", \"taskDuration\", \"tcMs\").summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Analytics`\n",
    "- higher number of tasks per stage\n",
    "- no failed or killed tasks\n",
    "- higher task and checkpoint duration than `GBT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+------------------+---------------+----------------+--------+\n",
      "|summary|meanNumCompleteTasks|meanNumFailedTasks|meanNumKilledTasks|meanTaskAttempt|meanTaskDuration|meanTcMs|\n",
      "+-------+--------------------+------------------+------------------+---------------+----------------+--------+\n",
      "|  count|                1392|              1392|              1392|            392|            1392|    1392|\n",
      "|   mean|   649.5999999999941|               0.0|               0.0|            0.0|             NaN|     NaN|\n",
      "| stddev|  134.80985058948727|               0.0|               0.0|            0.0|             NaN|     NaN|\n",
      "|    min|               547.2|               0.0|               0.0|            0.0|         23063.6|     NaN|\n",
      "|    25%|               547.2|               0.0|               0.0|            0.0|        533834.6|     NaN|\n",
      "|    50%|               561.6|               0.0|               0.0|            0.0|             NaN|     NaN|\n",
      "|    75%|               840.0|               0.0|               0.0|            0.0|             NaN|     NaN|\n",
      "|    max|               840.0|               0.0|               0.0|            0.0|             NaN|     NaN|\n",
      "+-------+--------------------+------------------+------------------+---------------+----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analytics_df.select(\"meanNumCompleteTasks\", \"meanNumFailedTasks\", \"meanNumKilledTasks\", \"meanTaskAttempt\", \"meanTaskDuration\", \"meanTcMs\").summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+------------------+--------------------+----------------+--------+\n",
      "|summary|meanNumCompleteTasks|meanNumFailedTasks|meanNumKilledTasks|     meanTaskAttempt|meanTaskDuration|meanTcMs|\n",
      "+-------+--------------------+------------------+------------------+--------------------+----------------+--------+\n",
      "|  count|               10152|             10152|             10152|                 696|           10152|   10152|\n",
      "|   mean|   526.5040189125624|1.3475177304964538|               0.0|0.002873563218390...|             NaN|     NaN|\n",
      "| stddev|   308.1893605657361|3.7889078664038935|               0.0|0.053567047660431966|             NaN|     NaN|\n",
      "|    min|                 0.0|               0.0|               0.0|                 0.0|         30546.0|914530.8|\n",
      "|    25%|               475.2|               0.0|               0.0|                 0.0|             NaN|     NaN|\n",
      "|    50%|               569.6|               0.0|               0.0|                 0.0|             NaN|     NaN|\n",
      "|    75%|               840.0|               0.0|               0.0|                 0.0|             NaN|     NaN|\n",
      "|    max|               840.0|              12.0|               0.0|                 1.0|             NaN|     NaN|\n",
      "+-------+--------------------+------------------+------------------+--------------------+----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analytics_check_df.select(\"meanNumCompleteTasks\", \"meanNumFailedTasks\", \"meanNumKilledTasks\", \"meanTaskAttempt\", \"meanTaskDuration\", \"meanTcMs\").summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_check_df.select(\"tcMs\", \"rddId\").distinct().sort(\"rddId\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|meanTcMs|rddId|\n",
      "+--------+-----+\n",
      "|     NaN| null|\n",
      "|     NaN|   14|\n",
      "|914530.8|   14|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analytics_check_df.select(\"meanTcMs\", \"rddId\").distinct().sort(\"rddId\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Plot 1*: total App runtime compared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_runtime = gbt_df.select(\n",
    "    F.min(\"submissionTime\").alias(\"start\"),\n",
    "    F.max(\"completionTime\").alias(\"end\"),\n",
    "    (F.max(\"completionTime\").cast(T.LongType()) - F.min(\"submissionTime\").cast(T.LongType())).alias(\"duration\")  # in seconds\n",
    ").select(\"duration\").collect()[0][0] / 60\n",
    "\n",
    "analytics_runtime = analytics_df.select(\n",
    "    F.min(\"submissionTime\").alias(\"start\"),\n",
    "    F.max(\"completionTime\").alias(\"end\"),\n",
    "    (F.max(\"completionTime\").cast(T.LongType()) - F.min(\"submissionTime\").cast(T.LongType())).alias(\"duration\")  # in seconds\n",
    ").select(\"duration\").collect()[0][0] / 60\n",
    "\n",
    "gbt_check_runtime = gbt_check_df.select(\n",
    "    F.min(\"submissionTime\").alias(\"start\"),\n",
    "    F.max(\"completionTime\").alias(\"end\"),\n",
    "    (F.max(\"completionTime\").cast(T.LongType()) - F.min(\"submissionTime\").cast(T.LongType())).alias(\"duration\")  # in seconds\n",
    ").select(\"duration\").collect()[0][0] / 60\n",
    "\n",
    "analytics_check_runtime = analytics_check_df.select(\n",
    "    F.min(\"submissionTime\").alias(\"start\"),\n",
    "    F.max(\"completionTime\").alias(\"end\"),\n",
    "    (F.max(\"completionTime\").cast(T.LongType()) - F.min(\"submissionTime\").cast(T.LongType())).alias(\"duration\")  # in seconds\n",
    ").select(\"duration\").collect()[0][0] / 60\n",
    "\n",
    "gbt_distinct_tcms = gbt_check_df.filter(~F.isnan(F.col(\"tcMs\"))).select(\"tcMs\").distinct()\n",
    "gbt_tcMs_sum_seconds = distinct_tcms.select((F.sum(F.col(\"tcMs\")) / 1000).alias(\"tcMsSumSeconds\")).collect()[0][0]\n",
    "\n",
    "analytics_distinct_tcms = analytics_check_df.filter(~F.isnan(F.col(\"tcMs\"))).select(\"tcMs\").distinct()\n",
    "analytics_tcMs_sum_seconds = analytics_distinct_tcms.select((F.sum(F.col(\"tcMs\")) / 1000).alias(\"tcMsSumSeconds\")).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "font = {'family' : 'arial',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 16}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "\n",
    "\n",
    "labels = ['gbt-9000000-10', 'gbt-9000000-10-checkpoint', 'analytics-1gb', 'analytics-1gb-checkpoint']\n",
    "\n",
    "\n",
    "runtimes = [gbt_runtime, gbt_check_runtime, analytics_runtime, analytics_check_runtime,]\n",
    "\n",
    "yerrs = [0, (gbt_tcMs_sum_seconds / 60), 0, (analytics_tcMs_sum_seconds / 60) ]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(14, 8)\n",
    "\n",
    "#\n",
    "ax.bar(labels, runtimes, width, label='App runtime (in minutes)', yerr=yerrs, color=[\"lightgreen\", \"green\", \"lightblue\", \"blue\"])\n",
    "\n",
    "# annotate the values to the bars\n",
    "for i,j in zip(labels,runtimes):\n",
    "    ax.annotate(str(format(j, '.2f')),xy=(i,j))\n",
    "\n",
    "ax.set_ylabel('Total Spark App runtime (in minutes)')\n",
    "ax.set_title('Total Spark App runtime compared')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_tcMs_sum_seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytics_tcMs_sum_seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Plot 2*: GBT `tcMs` over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_check_submission_time = gbt_check_df.select(\"submissionTime\").toPandas()[\"submissionTime\"]\n",
    "gbt_tc_ms = gbt_check_df.select(\"tcMs\").toPandas()[\"tcMs\"]\n",
    "\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "\n",
    "plt.plot(gbt_check_submission_time, tc_ms, '-o', color=\"green\")\n",
    "\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('checkpoint time (ms)')\n",
    "plt.title('Evolution of the checkpoint time (ms) over time')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Plot 3*: Analytics `tcMs` over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytics_check_submission_time = analytics_check_df.select(\"submissionTime\").toPandas()[\"submissionTime\"]\n",
    "analytics_tc_ms = analytics_check_df.select(\"tcMs\").toPandas()[\"tcMs\"]\n",
    "\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "\n",
    "plt.plot(analytics_check_submission_time, analytics_tc_ms, '-o', color=\"blue\")\n",
    "\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('checkpoint time (ms)')\n",
    "plt.title('Evolution of the checkpoint time (ms) over time')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Plot 3*: GBT: Failed and completed tasks per stage over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_submission_time = gbt_df.select(\"submissionTime\").toPandas()[\"submissionTime\"]\n",
    "\n",
    "gbt_complete_tasks = gbt_df.select(\"numCompleteTasks\").toPandas()[\"numCompleteTasks\"]\n",
    "gbt_failed_tasks = gbt_df.select(\"numFailedTasks\").toPandas()[\"numFailedTasks\"]\n",
    "\n",
    "gbt_check_complete_tasks = gbt_check_df.select(\"numCompleteTasks\").toPandas()[\"numCompleteTasks\"]\n",
    "gbt_check_failed_tasks = gbt_check_df.select(\"numFailedTasks\").toPandas()[\"numFailedTasks\"]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "\n",
    "plt.plot(gbt_submission_time, gbt_complete_tasks, color=\"lightgreen\", label=\"completed tasks\")\n",
    "plt.plot(gbt_submission_time, gbt_failed_tasks, color=\"red\", label=\"failed tasks\")\n",
    "\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('#tasks')\n",
    "plt.title('GBT: Failed and completed tasks per stage over time')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Plot 4*: GBT: Failed and completed tasks per stage over time (with checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_check_submission_time = gbt_check_df.select(\"submissionTime\").toPandas()[\"submissionTime\"]\n",
    "\n",
    "gbt_check_complete_tasks = gbt_check_df.select(\"numCompleteTasks\").toPandas()[\"numCompleteTasks\"]\n",
    "gbt_check_failed_tasks = gbt_check_df.select(\"numFailedTasks\").toPandas()[\"numFailedTasks\"]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "\n",
    "plt.plot(gbt_check_submission_time, gbt_check_complete_tasks, color=\"green\", linestyle=\"dashed\", label=\"completed tasks checkpoint\")\n",
    "plt.plot(gbt_check_submission_time, gbt_check_failed_tasks, color=\"red\", linestyle=\"dashed\", label=\"failed tasks checkpoint\")\n",
    "\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('#tasks')\n",
    "plt.title(' GBT: Failed and completed tasks per stage over time (with checkpoint)')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Plot 6*: Analytics: Failed and completed tasks per stage over time (with checkpoint)\n",
    "- number of tasks completed grows linearly without any failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytics_submission_time = analytics_df.select(\"submissionTime\").toPandas()[\"submissionTime\"]\n",
    "\n",
    "analytics_complete_tasks = analytics_df.select(\"numCompleteTasks\").toPandas()[\"numCompleteTasks\"]\n",
    "analytics_failed_tasks = analytics_df.select(\"numFailedTasks\").toPandas()[\"numFailedTasks\"]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "\n",
    "plt.plot(analytics_submission_time, analytics_complete_tasks, color=\"blue\", linestyle=\"dashed\", label=\"completed tasks checkpoint\")\n",
    "plt.plot(analytics_submission_time, analytics_failed_tasks, color=\"red\", linestyle=\"dashed\", label=\"failed tasks checkpoint\")\n",
    "\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('#tasks')\n",
    "plt.title(' Analytics: Failed and completed tasks per stage over time')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Plot 7*: Analytics: Failed and completed tasks per stage over time (with checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytics_check_submission_time = analytics_check_df.select(\"submissionTime\").toPandas()[\"submissionTime\"]\n",
    "\n",
    "analytics_check_complete_tasks = analytics_check_df.select(\"numCompleteTasks\").toPandas()[\"numCompleteTasks\"]\n",
    "analytics_check_failed_tasks = analytics_check_df.select(\"numFailedTasks\").toPandas()[\"numFailedTasks\"]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "\n",
    "plt.plot(analytics_check_submission_time, analytics_check_complete_tasks, color=\"lightblue\", linestyle=\"dashed\", label=\"completed tasks checkpoint\")\n",
    "plt.plot(analytics_check_submission_time, analytics_check_failed_tasks, color=\"red\", linestyle=\"dashed\", label=\"failed tasks checkpoint\")\n",
    "\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('#tasks')\n",
    "plt.title(' Analytics: Failed and completed tasks per stage over time (with checkpoint)')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
